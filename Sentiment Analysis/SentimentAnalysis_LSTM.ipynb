{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # For preprocessing\n",
    "import pandas as pd  # For data handling\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "# Tools for preprocessing input data\n",
    "from bs4 import BeautifulSoup\n",
    "from wordcloud import WordCloud\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "# Tools for creating ngrams and vectorizing input data\n",
    "from gensim.models import Word2Vec, Phrases\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning import Trainer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import gensim\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "%config Completer.use_jedi = False\n",
    "import spacy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def stemm_text(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return ' '.join([stemmer.stem(w) for w in text.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in text.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = df['review'].str.split(' \\n\\n---\\n\\n').str[0]\n",
    "T = T.str.replace('-',' ').str.replace('[^\\w\\s]','').str.replace('\\n',' ').str.lower()\n",
    "stop = stopwords.words('english')\n",
    "T = T.apply(lambda x: ' '.join(x for x in x.split() if  not x.isdigit()))\n",
    "T = T.apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stop))\n",
    "#T = T.apply(stemm_text)\n",
    "#T = T.apply(lemmatize_text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe-100d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sentence2Vec(T,embedding_dim = 100,max_length = 120):\n",
    "    glove_path = '../input/glove6b100dtxt/glove.6B.100d.txt'\n",
    "    path = glove_path\n",
    "    tokenizer = Tokenizer()\n",
    "    text=T\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    word_index=tokenizer.word_index\n",
    "    print(\"number of word in vocabulary\",len(word_index))\n",
    "    vocab_size = 5000\n",
    "    trunc_type = 'post'\n",
    "    oov_tok = '<OOV>'\n",
    "    padding_type = 'post'\n",
    "    #print(\"words in vocab\",word_index)\n",
    "    text_sequence=tokenizer.texts_to_sequences(text)\n",
    "    text_sequence = pad_sequences(text_sequence, maxlen=max_length, truncating=trunc_type,padding=padding_type)\n",
    "    print(\"word in sentences are replaced with word ID\",text_sequence)\n",
    "    size_of_vocabulary=len(tokenizer.word_index) + 1\n",
    "    print(\"The size of vocabulary \",size_of_vocabulary)\n",
    "    embeddings_index = dict()\n",
    "\n",
    "    f = open(path)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    embedding_matrix = np.zeros((size_of_vocabulary, embedding_dim))\n",
    "\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    text_shape = text_sequence.shape\n",
    "    X_train = np.empty((text_shape[0],text_shape[1],embedding_matrix.shape[1]))\n",
    "    for i in range(text_sequence.shape[0]):\n",
    "        for j in range(text_sequence.shape[1]):\n",
    "            X_train[i,j,:] = embedding_matrix[text_sequence[i][j]]\n",
    "    print(X_train.shape)\n",
    "\n",
    "    return X_train,embeddings_index,word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,embeddings_index,word_index = Sentence2Vec(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix = np.zeros((len(word_index)+1, 100))\n",
    "\n",
    "for word, index in word_index.items():\n",
    "  embedding_vector = embeddings_index.get(word)\n",
    "  if embedding_vector is not None:\n",
    "    emb_matrix[index, :] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "\n",
    "z = pca.fit_transform(emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame()\n",
    "df3[\"comp-1\"] = z[:,0]\n",
    "df3[\"comp-2\"] = z[:,1]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.scatterplot(x=\"comp-1\", y=\"comp-2\",data=df3).set(title=\"Glove PCA projection\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.sum(X_train,axis=1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "\n",
    "z = pca.fit_transform(X)\n",
    "df3 = pd.DataFrame()\n",
    "df3[\"comp-1\"] = z[:,0]\n",
    "df3[\"comp-2\"] = z[:,1]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.scatterplot(x=\"comp-1\", y=\"comp-2\",hue=df['sentiment'].tolist(),palette=sns.color_palette(\"hls\", 2),data=df3).set(title=\"Glove PCA projection\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style = \"darkgrid\" , font_scale = 1.2)\n",
    "sns.countplot(df.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20)) # Positive Review Text\n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(T))\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sentiment.replace(\"positive\" , 1 , inplace = True)\n",
    "df.sentiment.replace(\"negative\" , 0 , inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['sentiment'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.Tensor(X_train)\n",
    "train_targets = torch.Tensor(y_train).type(torch.LongTensor)\n",
    "val_features = torch.Tensor(X_val)\n",
    "val_targets = torch.Tensor(y_val).type(torch.LongTensor)\n",
    "\n",
    "\n",
    "trainDataset = TensorDataset(train_features, train_targets)\n",
    "valDataset = TensorDataset(val_features, val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitLSTM(pl.LightningModule):\n",
    "    def __init__(self,num_classes,dimension,hidd_dim):\n",
    "        super(LitLSTM, self).__init__()\n",
    "        \n",
    "        self.hidd_dim = hidd_dim\n",
    "        self.dim = dimension\n",
    "        \n",
    "        self.MHA = nn.MultiheadAttention(self.dim,2,dropout=0.5,batch_first=True,kdim=self.dim,vdim=self.dim)\n",
    "        \n",
    "        self.LSTM = nn.LSTM(input_size=self.dim,\n",
    "                            hidden_size=self.hidd_dim,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        \n",
    "        self.model = nn.Sequential( nn.Dropout(p=0.5),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Linear(self.hidd_dim, num_classes),\n",
    "                                    nn.LogSoftmax(dim=1))\n",
    "        \n",
    "        # add metrics\n",
    "        self.train_acc = torchmetrics.Accuracy()\n",
    "        self.train_f1 = torchmetrics.F1(average=\"micro\")\n",
    "        self.val_acc = torchmetrics.Accuracy()\n",
    "        self.val_f1 = torchmetrics.F1(average=\"micro\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x,_ = self.MHA(x,x,x)\n",
    "        out, (final_hidden_state, final_cell_state) = self.LSTM(x)\n",
    "        out = self.model(final_hidden_state[-1])\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        # Forward pass\n",
    "        outputs = self.forward(images)\n",
    "        lossfn = nn.NLLLoss()\n",
    "        loss = lossfn(outputs, labels)\n",
    "        \n",
    "        y_pred = torch.exp(outputs)\n",
    "        #y_pred = output.data.max(1, keepdim=True)[1]\n",
    "        acc = self.train_acc(y_pred, labels)\n",
    "        f1 = self.train_f1(y_pred, labels)\n",
    "        # just accumulate\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_accuracy\", acc)\n",
    "        self.log(\"train_f1\", f1)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        # use key 'log'\n",
    "        return {\"loss\": loss, 'log': tensorboard_logs}\n",
    "\n",
    "    # define what happens for testing here\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        trainDataLoader = DataLoader(trainDataset, num_workers=4,batch_size=BS, shuffle=True,drop_last=True)\n",
    "\n",
    "        return trainDataLoader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        valDataLoader = DataLoader(valDataset, num_workers=4,batch_size=BS,shuffle=False,drop_last=True)\n",
    "        \n",
    "        return valDataLoader\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        # Forward pass\n",
    "        outputs = self.forward(images)\n",
    "        lossfn = nn.NLLLoss()\n",
    "        loss = lossfn(outputs, labels)\n",
    "        \n",
    "        pred = torch.exp(outputs)\n",
    "        #pred = output.data.max(1, keepdim=True)[1]\n",
    "        self.val_acc.update(pred, labels)\n",
    "        self.val_f1.update(pred, labels)\n",
    "\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return {\"val_loss\": loss}\n",
    "     \n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        # compute metrics\n",
    "        train_accuracy = self.train_acc.compute()\n",
    "        train_f1 = self.train_f1.compute()\n",
    "        # log metrics\n",
    "        self.log(\"epoch_train_accuracy\", train_accuracy)\n",
    "        self.log(\"epoch_train_f1\", train_f1)\n",
    "        # reset all metrics\n",
    "        self.train_acc.reset()\n",
    "        self.train_f1.reset()\n",
    "        print(f\"\\ntraining accuracy: {train_accuracy:.4}, \"\\\n",
    "        f\"f1: {train_f1:.4}\")\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # outputs = list of dictionaries\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        val_accuracy = self.val_acc.compute()\n",
    "        val_f1 = self.val_f1.compute()\n",
    "        # log metrics\n",
    "        self.log(\"val_accuracy\", val_accuracy)\n",
    "        self.log(\"val_loss\", avg_loss)\n",
    "        self.log(\"val_f1\", val_f1)\n",
    "        # reset all metrics\n",
    "        self.val_acc.reset()\n",
    "        self.val_f1.reset()\n",
    "        print(f\"\\nvalidation accuracy: {val_accuracy:.4} \"\\\n",
    "        f\"f1: {val_f1:.4}\")\n",
    "        \n",
    "        tensorboard_logs = {'avg_val_loss': avg_loss}\n",
    "        # use key 'log'\n",
    "        return {'val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeding for torch, numpy, stdlib random, including DataLoader workers!\n",
    "seed_everything(123, workers=True)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    stopping_threshold=1e-5,\n",
    "    divergence_threshold=9.0,\n",
    "    check_finite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitLSTM(dimension=X_train.shape[2],hidd_dim=128,num_classes=2)\n",
    "trainer = Trainer(accelerator='gpu',devices=1,max_epochs=100,log_every_n_steps=8,callbacks=[early_stopping])\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predsentiment(rev):\n",
    "    print(rev)\n",
    "    print(\"==========================================================================================================================================\")\n",
    "    dftest = pd.DataFrame(columns=['review'])\n",
    "    dftest = dftest.append({'review': rev}, ignore_index=True)\n",
    "    T = dftest['review'].str.split(' \\n\\n---\\n\\n').str[0]\n",
    "    T = T.str.replace('-',' ').str.replace('[^\\w\\s]','').str.replace('\\n',' ').str.lower()\n",
    "    stop = stopwords.words('english')\n",
    "    T = T.apply(lambda x: ' '.join(x for x in x.split() if  not x.isdigit()))\n",
    "    T = T.apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stop))\n",
    "    x,_,_ = Sentence2Vec(T)\n",
    "    print(\"==========================================================================================================================================\")\n",
    "    X_test = torch.Tensor(x)\n",
    "    with torch.no_grad():\n",
    "        output = model(X_test)\n",
    "        output = torch.exp(output)\n",
    "        #print(output)\n",
    "        index = output.data.cpu().numpy().argmax()\n",
    "        result = list(np.around(output.data.cpu().numpy()*100,1))\n",
    "        print(\"PREDICTION PROBABILITY = \",result)\n",
    "        strn = \"POSITIVE\"\n",
    "        if(index==0): strn = \"NEGATIVE\"  \n",
    "        print(\"PREDICTED CLASS = \",strn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review = \"You can compare race 3 to tamil movies. Not a single realistic thing or fight. Everything has been exaggerated.There is all this stupid suspense, but to make the movie look like more suspenseful and complicated they have generated too many relations and all of them are predictable.I would suggest if you are a fan of Salman khan then do not watch the movie coz you will be disappointed. But if you are a fan of race series then do watch it for fun or anything you feel like and digest it someway because race 4 might be on its track. And in other situation if you are none then don't waste your time and money here.\"\n",
    "\n",
    "predsentiment(test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "220fae48ee6aa06a15b0a194b915ad59277734e5bd52415b52439bc6d564df3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
